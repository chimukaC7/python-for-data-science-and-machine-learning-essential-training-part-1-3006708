Pearson correlation analysis
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Let's talk about parametric correlation analysis. Parametric correlation analysis is a method you can use to find correlation between linearly related continuous numeric variables. Don't worry if you don't exactly understand what that means because I'm going to show you how to figure this out in just a minute. First, I want to explain one important point about correlation. Correlation does not imply causation. Let me explain. Imagine you're a doctor studying regional obesity trends. You have two data sets: One on store size reported by zip code, and two on national obesity prevalence broken down by zip codes. In the course of your investigation, you apply the Pearson correlation method, that's the method I'm about to show you, and you find that there's a very strong positive correlation between grocery store size and obesity. The bigger the grocery stores, the more obesity there tends to be. Of course, the size of the store doesn't cause obesity, but they're correlated, and that correlation is quantifiable through the Pearson Method. Pearson correlation is measured by the correlation coefficient, R. If you have a Pearson R that's close to one, then that's a strong positive relationship. And if you had an R value that is close to negative one, then you've got a strong negative relationship. If you have an R value equal to zero, or close to it, then you're basically seeing that your variables are not linearly correlated. Now, the Pearson correlation assumes that your data is normally distributed, that you have continuous numeric variables, and that your variables are linearly related. A really important note that I wanted to add here is how do you use the Pearson correlation? So it's safe to use Pearson correlation to uncover linear relationships between variables, but you can not use it to rule out the possibility of non-linear relationships between variables. For this demonstration, we're going to be bringing in our standard libraries, pandas and numpy, but also please note that I've imported matplotlib and seaborn, as well as the rcParams. We're also going to be using scipy in this demonstration. So all of these are already loaded in our notebook, and I've also preloaded the empty cars data set that we've been working with, and set the plotting parameters for matplotlib. We covered all of these things in previous lectures, but the one thing I want to point out here is that we are importing Pearson R from the scipy stats package. So you have to first start off just by running these. And like I said, empty cars is ready to go. So all we need to do is run this to load it into our environment. Cool. So the column names are all set, and let's just go ahead and start by generating a pairplot using the seaborn library. To do that, we'll use the pairplot function, so that's sns.pairplot, and we'll pass in cars data frame and run this. Move it up a bit. It's just thinking for a little while. You can tell what Python's doing just by seeing this moving blue scroll at the top, and also this icon here saying how long it's taken for Python to process the request. Okay, wow. So we have a lot of data here that's been plotted out for us. And as you can see, if you were to count them up, we actually have 11 numeric variables in the cars data set. This basically takes up a lot of space. I went ahead and selected some variables for our analysis, and I'll go ahead and generate a scatter plot matrix of those in order to show you what about them is desirable for the Pearson correlation. And I'm going to take you over into another screen to explain really quickly. But before I do that, let's just make this second scatter plot. So we'll call it X, and we'll set X equal to our cars data frame, but we only want to select four columns, which are mpg, hp, qsec, and wt. Add the single quotes here. And then, again, we use the pairplot function, so that's sns.pairplot, pass in X, and run this. Okay, so that was a lot faster, and here we have a smaller pairplot. Now let me take you over to the other screen to explain what all this means. So let's consider the model assumptions for the Pearson correlation analysis. Pearson correlation assumes that your data is normally distributed, that variables are linearly related, and that the variables are continuous numeric variables. Let's look here at the normally distributed requirement. A normally distributed requirement is going to give a shape like a bell curve in a histogram. I wouldn't say that all these variables are exactly normally distributed, but they could possibly be close enough in order to generate some sort of correlation using the Pearson correlation method, so I'm going to go with these. Now, let's look at the requirement for a linear relationship. Do these variables have a linear relationship between them? In other words, does one increase while the other decreases? Based on the shape of the distribution of points between the variables, it looks like most of these have a distribution that could be at least close to linear, so I'm going to test them out with the Pearson correlation method. The last requirement is that the variables be continuous numeric variables. The best way for me to show you why I think that these are continuous numeric variables is to show you what a variable looks like when it's not a continuous numeric variable. If you look over at the scatterplot on the right, these variables over here are not continuous numeric variables. These are categorical variables because they can only assume a fixed number of positions, like we just discussed in the last section. So this variable can assume one of two values. That makes it a binomial variable. In the gear variable, it can assume three values; three, four, or five. That makes it a multinomial variable. These are not continuous numerical variables. When you see continuous numerical variables, the scatterplot of the variables is much more randomly and evenly distributed. The end conclusion here is that the variables that are shown on the right would not qualify for the Pearson R correlation analysis. Okay, great. So let's get back to our coding demonstration and use scipy to calculate Pearson correlation coefficients. Now, let's look at how to use scipy to calculate the Pearson correlation coefficient. Okay, so let's start by creating some variables we can use here. So we'll create an mpg variable, and we'll set that equal to cars.mpg. And then let's create an hp variable that's equal to cars hp. We'll create a qsec variable and we'll set that equal to cars qsec. And then, a wt variable, which will be directly from our cars data frame, the weight variable here. Okay. So let's start first by taking the Pearson R coefficient of the mpg and hp variable pair. So to do that, we're going to say pearsonr_coefficient and P value. We're going to set these equal to the Pearson R function, and we'll pass in our mpg and our hp. And then, let's print out the label. So we'll say print, and let our label be Pearson R correlation coefficient. Okay. And then, say %0.3f. And then, give another percentage sign, create a tuple here, and pass in our Pearson R coefficient object. Okay, I'm going to look this over really quick for any typos. Okay, yeah, so one issue is that I needed to close out the string here after the F, so I'm going to add a single quote and remove the single quote from there, and then we should be good to go. So let's run this. And then, what I'm going to do to calculate the Pearson R for the other variable pairs is just to copy this little chunk of code and paste it down here, and just change the variables out. Once we have the Pearson R coefficients, then we will discuss. So the second variable pair is going to be mpg and qsec. And the third variable pair will be mpg and weight. So let me run this. And this. Okay, great. So now we have our Pearson R values. Let's just look at what this means. Based on the Pearson correlation coefficient of these three variable pairs, the mpg weight variable pair appears to have the strongest linear correlation. The mpg qsec variable pair has a moderate degree of linear correlation. And you may be wondering, "Well, what do I do "with this information once I have it?" When you're doing machine learning, or other forms of advanced statistical analysis, these models often have assumptions that either the features are independent of one another or that they exhibit a degree of correlation, and you're going to see that later in this course. So you can use the Pearson R correlation coefficient to establish whether or not your variable pairs meet the requirements of more advanced models. Now, that you've seen the long form way of calculating the Pearson R value, let me show you some shortcuts. We will start by using pandas to calculate Pearson R correlation coefficient. So let me just notate that here. You can also generate some Pearson R statistics by using this corr method, so let's do it really quick. Using pandas, you can also generate Pearson R statistics by using the corr method. So let's do that real quick. We'll say that corr here is equal to x.corr, called the corr method. And then we'll print this out. And as you can see, it's really quickly generated all of the Pearson R values for each of the variable pairs in our smaller subset. The last way you can do this is using seaborn, and that would be with its heat map function. So we'll just say sns.heat map, and then we'll pass in our corr variable, and then we'll create some tick labels. So our xticklabels will be equal to the column values in our corr data frame. So we're going to say corr.column.values, and then our ytick labels will be equal to the columns in the corr data frame. So corr.columns.values, and then we run this. That looks nice, but what does it mean? Well, the darker shades of red indicate a strong degree of positive correlation, as you can see from the legend. Based on what we see, the hp weight variable pair has the highest degree of positive linear correlation. Judging by the darker hues in the grid, the mpg weight variable pair appears to have the strongest degree of negative linear correlation. You'll of course see here that when mpg is plotted against itself, then it has an absolute value of one. It correlates 100% with itself, that's why these are solid cream colors here. And then the sort of fuchsia color here, the weight qsec variable pair is not linearly correlated. Keep in mind, that doesn't mean there's no correlation between these variables whatsoever. In the next video, I'm going to show you some methods you can use to establish correlation between non-linearly related variables.


Pearson correlation analysis is like checking if there's a dance between two sets of numbers. Imagine two dancers on a dance floor: if one moves forward and the other also moves forward in sync, they're positively correlated, meaning they move together in the same direction. If one moves forward and the other moves backward, they're negatively correlated, meaning they move in opposite directions. However, if one dancer moves randomly without any predictable pattern in relation to the other, then there's no correlation between them.

It's crucial to remember that just because two things move together, it doesn't mean one causes the other to move. For example, just because ice cream sales and the number of people swimming increase at the same time doesn't mean buying more ice cream causes more people to swim. They're both happening together because it's hot outside, not because one causes the other.

The Pearson correlation gives us a number, called the R value, to describe how closely the two sets of numbers dance together. An R value close to 1 means they're in close step in a positive way (moving together), an R value close to -1 means they're in close step but in opposite directions (one goes up, the other goes down), and an R value around 0 means they're not really dancing together at all.

So, Pearson correlation helps us see the relationship between two things, but it's up to us to understand the context and remember that moving together doesn't always mean one causes the other to move.


