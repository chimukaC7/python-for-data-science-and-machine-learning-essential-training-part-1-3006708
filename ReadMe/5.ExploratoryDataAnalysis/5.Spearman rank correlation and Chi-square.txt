Spearman rank correlation and Chi-square
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Let's talk about nonparametric correlation analysis. You can use nonparametric correlation analysis to find correlation between categorical nonlinearly, non-normally distributed variables. For an example of where nonparametric correlation analysis could be useful, imagine that you're a social scientist that studies smoking habits. You'll use a non-parametric correlation analysis like Spearman's rank to test the population for a correlation between income as a bracket and cigarette consumption of smokers. You find that higher income individuals are much more likely to smoke cigarettes than lower income people. I'm about to show you how to use Spearman's rank correlation and chi-square tables to establish correlation between categorical variables. The Spearman's rank correlation method works on ordinal variables. In case you don't know what that is, an ordinal variable is a numeric variable that is able to be categorized. The Spearman's rank method converts ordinal variables into variable pairs and then calculates an R correlation coefficient by which to rank their variable pairs according to the extent of their correlation. If that doesn't make much sense for you now, don't worry at all because I'm going to show you what this means in the coding demonstration to come. But first, let's talk about the R values for Spearman's rank test. Similar to Pearson correlation, if you get an R value that is close to one, then you are seeing a strong positive relationship. Whereas if you get an R value that's close to negative one, then you're seeing a strong negative relationship. If your R value is close to zero, you are seeing that there is either a weak relationship or no relationship whatsoever. In terms of assumptions, the Spearman's correlation assumes that your variables are ordinal. In other words, they are numeric but able to be ranked like categorical variable. It also assumes that your variables are related nonlinearly. Lastly, it assumes that your data is non-normally distributed. Don't worry about these too much right now though, because in the coding demonstration, I'm going to show you how to examine your variables and find out whether they meet these assumptions. You can also use the chi-square test to see if non-linear variables are independent of one another. The null hypothesis of this test is that the variables are independent of one another. So if you have a p value of less than 0.05, you would reject the null hypothesis and conclude that the variables are correlated. If you had a p value greater than 0.05, you'd accept the null hypothesis and conclude that the variables are independent of one another. In terms of the assumptions of the chi-square test, you just want to make sure your variables are categoric or numeric. If you have numeric variables, then you're going to need to make sure that you have binned them. And in case you don't know what binning is, now is a great time to get familiar with that term. As an example, imagine you had a variable that had values between zero and 100. That's a numeric variable. As an example of binning, you could break up that variable into 10 separate groups, 10 groups of 10. And then within these 10 groups of 10, you would just put your data into different categories according to its numeric values, like this. Now that you know what binning is, let's move on to the coding demonstration portion of this section. Okay, so for this demonstration, we're bringing in our standard libraries, pandas and numpy, but please note that I also imported matplotlib and seaborn as well as rcParams. And we're going to be using scipy in this demonstration. So all of these are already loaded into our notebook. I've also preloaded the mtcars data set that we've been working with and set the plotting parameters for matplotlib. We covered all of these in previous lectures. The one thing I wanted to point out specifically here is that we are importing spearmanr from scipy.stats package. You can see that here. So you just start by running this. (keyboard taps) And as I said, the data sets are already loaded for you, but what you need to do is just run this so we can take a look at the head of the dataframe, the cars dataframe. Okay, great. So we have a little preview of what's inside the cars data set. Now, let's just generate a quick pairplot from seaborn. To do that, we'll say sns.pairplot, and we'll pass in cars, and run this. Let it think for a little while. Okay, great. So we've got our scatter plot matrix, but as you can see, since there's so many variables in the dataset, it's pretty hard to visually see what's going on. So I went ahead and selected some variables for our demonstration here. So let's just make a scatter plot matrix of these so I can show you why I chose them. We'll call the subset x and then we'll just select some variables from our cars dataframe. We'll take the cyl variable, the vs variable, the am variable and the gear variable. And then we will call the pairplot function. Set sns.pairplot, and then we'll pass in our x object, and run this. All right, so there you have it. Now let me explain why I chose these variables. The first thing I looked at is are these ordinal variables? Well, if they're numeric but able to be ranked into categories, then yes, all of these variables are numeric. And they each assume only a set number of possible values. So yes, these variables are ordinal. Are these variables related nonlinearly? Well, based on this quick glimpse, I don't see any linear relationships between the variables. So hopefully, yes. Lastly, is the data distribution of each variable non-normal. Judging from the histogram here, I'd say yes. Based on this reasoning, I decided to test the variables cylinder, vs, am and gear. So next steps. Let's just go ahead and isolate each of these variables. So we'll have cyl, and that's going to be equal to the cyl column of our car dataframe. And vs is equal to our vs column. Am is equal to our am column. And gear is equal to our gear column. Okay, so we isolated our variables here. So now, let's go ahead and let's just create some outputs for our Spearman rank correlation. Let's do that by saying spearmanr_coefficient, (keyboard taps) underscore coefficient. P_value, and most of these equal to the spearmanr function, and we'll pass in our cyl and vs variable pair here. Need to change this order here to YL. And then we'll also print out a label so that we can really understand what our test is telling us. To do that, we'll just write print, and then we'll create a string which reads, Spearman Rank Correlation Coefficient %0.3f. Spearman Rank Correlation Coefficient %0.3f. And then close out the string. And then we will write out our results, which is going to be this placeholder here, which is spearmanr_coefficient. And check the syntax real quick. Looks okay. I'm going to run this. Okay, so now what I'm going to do is I'm going to copy this code here so that we can use it to calculate spearmanr for the other variable pairs. (mouse clicks) So the second variable pair here should be cyl versus am. And then the third variable pair will be cyl versus gear. Okay, so we have these, let's just run them real quick. So based on the Spearman's rank correlation coefficient of these three variable pairs, the cylinder vs variable pair appears to have the strongest correlation. The other variable pairs do show some correlation, but only a moderate amount. That was pretty easy. Now let's look at the chi-square test for independence. To implement the chi-square test, we first need to start off by creating a cross tab. We'll call it table, and we'll say cross tab equal to pd.crosstab. We'll pass the cylinder and am variables into this function. And then what we need to do is we need to import our chi-square function. So that comes from the scipy library stats module. So we'll do an import by saying, from scipy.stats import chi2_contingency. Okay, now we have what we need to actually implement a chi-square test. Let's write some placeholders for our output. We'll put chi2 as the first placeholder, and then p and then dof, and lastly, expected. And we'll set these equal to the output of our chi contingency function. So chi2_contingency. And then we need to pass in our table. This is the table we just created, but we want to pass in only the values. So we'll say table.values. And then let's print this out. We need a label so that it all makes more sense. So let that label be Chi-square Statistic %0.3f, close the string. And then we want to print out the p value, p_value label. Okay, so we should actually close the string here. This is all one label. And add a percentage sign. And then our actual values that are generated by the task. So that's going to be chi2 and p placeholders here. Okay, and then we'll run this. Okay, so it looks like there's a typo here. Let's go ahead and take out this comma and then also just fix the spelling here, table. And then run this. So what we're getting here is a chi-square statistic for cylinder am variable pair. Let's just generate a few more chi-square statistics on other variable pairs. To do that, I'm just going to copy this code over. (keyboard taps) And then I'll change out the variable pairs. So let's change this for vs. And then we'll switch out the am on the third variable pair for gear. Run this. So I'll move this up so we can see the results. Remember, with the chi-square test, we need a p value greater than 0.05 in order to conclude that the variables are independent of one another. Based on what I see here, none of the p values are greater than 0.05, so we must reject the null hypothesis and conclude that the variable pairs are correlated. That's exactly how you would use the chi-square test. Now, let's look at how to do extreme value analysis for outliers.


Sure, let's break these concepts down into simpler terms.

Spearman Rank Correlation is a way to see if there's a relationship between two sets of data when the data isn't perfectly smooth or linear. Imagine you're comparing the rankings of your favorite movies with a friend's rankings. Spearman's method helps you understand how similar your rankings are. If both of you rank the same movie as number one, another as number two, and so on, your rankings have a strong positive relationship. If your top movie is your friend's least favorite and vice versa, that's a strong negative relationship. If there's no clear pattern between your rankings, then there's no relationship.

Chi-square Test is used when you want to see if there's a connection between two categories. Let's say you're looking at whether the color of a car affects its likelihood of being in an accident. You categorize cars by color and whether they've been in an accident or not. The Chi-square test helps you figure out if there's a link between car color and accidents. If the test shows that color and accidents are related, maybe red cars have more accidents, you've found a correlation. If not, then the two are independent of each other.

In both cases, you're trying to find out if there's a relationship or connection between two things, but you use Spearman when you're dealing with rankings, and Chi-square when you're looking at categories.


Spearman's rank correlation coefficient
R=1 -> strong positive relationship
R=0 -> not correlated
R=-1 -> strong negative relationship

Chi-Square tables test for independence
p < 0.05 -> reject null hypothesis and conclude that the variables are correlated
p > 0.05 -> accept null hypothesis and conclude that the variables are independent