Cleaning and treating categorical variables
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Let's take another look at categorical variables and why we might need to treat categorical variables as well as the options we have for treating them. As you recall, a categorical variable is a type of variable that can take on only a limited or fixed number of possible values. For example, fruit types is a categorical variable as there are only a limited number of types of fruits. Say for example, apples, oranges, lemons, there's not an infinite number of fruit types, so it's categorical. In the field of machine learning, it's common to come across categorical variables when addressing data science challenges. Typically, machine learning algorithms are not equipped to directly process categorical data. Therefore, we have to transform this type of data into numerical formats that are compatible with machine learning algorithms. This transformation can be done through various methods, including label encoding, one-hot encoding, among others. The conversion of categorical variables into numerical forms is known as encoding. In this coding demonstration, I will demonstrate two common encoding techniques, label encoding and one-hot encoding. We'll be using the scikit-learn library to implement these encodings. During the coding demonstration, we will explore how to transform categorical variables into formats that are interpretable by machine learning models. Let's get started importing the required libraries for cleaning and treating categorical variables, I've already imported numpy in Pandas, and now we'll also import the other required libraries. One thing I wanted to point out here is that from sklearn, the preprocessing module, we're importing LabelEncoder and OneHotEncoder. We will need both of these functions for this demonstration. Here you can see I've already created the dataset that we will work with. Here are the columns, and as you can see in the gender column, it has some missing values. So let's convert this dataset to a data frame and then print it out to get started working with it. We'll call the data frame df and we'll say df is equal to data frame constructor, and then we'll pass in the dataset, which is called data, and then print this out. There are different ways to handle missing data and categorical variables. If there are just a few missing values, then we can drop the rows that contain the missing values. Or if there are a lot of missing values in a column, then we can drop that column altogether. We can also replace the missing values with the most frequent value of that column or row. Of course, let's start by adopting the most logical way of handling missing categorical data points. If you think about it here with this dataset, you cannot fill the missing values in the gender column with the most frequent values because there's a chance of assigning the wrong gender to a person. So in this case, we'll have to just drop the gender column. To do that, we'll say df is equal to df.drop, call the drop method, and then here we'll pass the column name gender that we want to drop as the first perimeter. And in the second perimeter, we will pass Axis equal to 1, which refers to the columns of the data frame. And we'll print this out, and as you can see, the gender column is dropped from the data frame. Next, let's try to represent the information that's contained within the names field such that it's represented by categorical numerical data. This will require a two-step approach. First, we'll use label encoding to create a numerical representation of each value in the names field. And then after that, we'll use OneHotEncoder to convert each value of these numerical values into its own unique categorical column. For labeling coding, we'll use sklearn's label encoding function to transform the names into categorical numerical values. How label encoding function works is that it encodes the target labels, in this case, names, with values between zero and N minus one, where N is the total number of unique values in the variable. To do this, first, we'll create an object of the label encoder. So we'll say label_encoder and we'll set this equal to label encoder. Then we'll call the label encoders a fit function and pass column with a categorical data in it. So we'll say label_encoder.fit and we will pass in df['names'] because this is the variable that we want to have transformed. What this does is that it's going to create a numerical mapping that maps the names labels to categorical numerical values. So I'll run this. Now we will generate the encodings of the categorical variable by calling the transform method off of the label encoding class. So let's call this label encoded names, label_encoded_names and we'll set this equal to our label_encoder and we'll call the transform method off of that, and we'll pass in again, we'll pass in our names column. So df['names'] and print this out. What this is going to do is it's going to generate their encodings. So here, you see the output, the numerical encodings have been generated for the names column. Now we need to transform the categorical data using one-hot encoding. In one-hot encoding, each categorical value is converted into a new categorical column and it is assigned a binary value 0 or 1 for whether the data point is true or false for that value. Let's just try it out so you can see how it works. First, we'll create the object of class onehot_encoder. So we'll say OneHotEncoder and we want to pass a parameter that says sparse_output is equal to false. And what we'll do is we'll call this the onehot_encoder and then run this, then we'll just call the fit method off of the class onehot_encoder. So to do that, we'll say onehot_encoder.fit and we'll pass in our data frame names column. It looks like I have a typo. I'm missing a set of brackets here, so I'm going to go ahead and add those, and run this again. What this has done is that it's performed one-hot encoded mapping of the categorical values. Now let's transform the data by calling the transform function and passing the categorical values. We'll call this onehot_encoded_names. So onehot_encoded_names and we'll set it equal to our onehot_encoder. We'll call the transform method and we'll pass in our data frame, names column, and run this. Lastly, let's just save this as a data frame. So we'll say one-hot encoded data frame, onehot_encoded_df. We'll call the data frame constructor. First, we'll pass the output encodings, so we'll say onehot_encoded_names here. Then we'll pass the mappings of all the columns as column names. So we'll say columns is equal to onehot_encoder.categories. Next, we'll assign the original column and the data frame as names. So we'll say onehot_encoded_df and we'll select the names and we'll set this equal to df[['names']] column. And we'll print this out by saying onehot_encoded_df, I'm just looking at the syntax real quick before running this. And okay, so now we have transformed our names into a set of categorical numerical variables that are represented in binary format here, as you can see. So each of the names has been represented as its own categorical variable in the dataset. And then for where the value is actually true for that data point, which is actually Steve in the original dataset, that value gets a 1.0. Every other value in that column gets a 0.0. And that's the basics of label encoding and one-hot encoding. Next, let's look at transforming dataset distributions.


The video “Cleaning and treating categorical variables” provides essential insights into handling categorical data in machine learning projects, focusing on:

Understanding Categorical Variables: It explains what categorical variables are and why they need to be treated or transformed before being used in machine learning models, as most algorithms require numerical input.

Encoding Techniques: The video demonstrates two common techniques for converting categorical variables into numerical formats: label encoding and one-hot encoding, using the scikit-learn library. This is crucial for preparing your dataset for machine learning algorithms.

Handling Missing Values in Categorical Data: It discusses strategies for dealing with missing values in categorical variables, including dropping the variable or replacing missing values with the most frequent category, highlighting the importance of data preprocessing in machine learning.


These takeaways are particularly useful for transitioning into a role as a Machine Learning Engineer, as they cover foundational skills in data preprocessing and feature engineering.


Why might categorical variables need to be treated?
-to shift into numerical representations for analysis