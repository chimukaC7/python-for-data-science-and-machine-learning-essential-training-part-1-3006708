Here are the key takeaways from the "Asynchronous scraping" video:

Efficiency: Asynchronous web scraping allows multiple requests to be made simultaneously, reducing CPU idle time and speeding up data extraction.
Libraries: The video demonstrates the use of aiohttp for making asynchronous requests and asyncio for writing concurrent code.
Implementation Steps:
Install necessary libraries (aiohttp, asyncio, nest_asyncio).
Define asynchronous functions to scrape and save links.
Use asyncio.create_task to run tasks concurrently and asyncio.gather to manage multiple URLs.


Efficiency of Asynchronous Scraping: Asynchronous web scraping allows multiple requests to be made simultaneously, reducing CPU idle time and speeding up the data extraction process.
Libraries Used: The video demonstrates the use of aiohttp for making asynchronous requests and asyncio for writing concurrent code in Python.
Implementation Steps: The video walks through installing necessary libraries, setting up an asynchronous function to scrape links, and saving them to a CSV file, highlighting the importance of handling long waiting operations efficiently.



These techniques help in handling large amounts of data more efficiently and effectively.
These techniques can significantly enhance the performance of your web scraping tasks.




Asynchronous scraping
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Let's look at asynchronous web scraping and how we can use this method to speed up the process of extracting data. A normal web scraper can take a lot of time to extract large amounts of data, or in some cases, it might just fail altogether. This usually happens when a web scraper makes a request and then waits for the response before jumping to the next request. During this time, the CPU remains idle, which wastes a lot of CPU time. Circumventing this process, asynchronous web scraping allows us to make multiple requests simultaneously without waiting for the response of each request. It also reduces CPU idle time. In the demonstration of asynchronous scraping that's coming up, we will use the aiohttp and the asyncio libraries. Aiohttp is a client and server side library that allows us to make asynchronous requests. On the other hand, asyncio is a Python library, which is used to write concurrent code. Let's take a look at it inside of Python. For this demonstration, we're going to be working with new libraries. One is aiohttp, and the other is asyncio. So just to be careful, let's go ahead and do a pip install of each of those to make sure you've got them installed in your environment. So we'll start with pip install aiohttp, and run this. And then let's do pip, pip install asyncio, and run this. Okay, great. Now, your notebook is already coming preloaded with the libraries that you'll need. And we'll mainly be working with aiohttp and asyncio, but also with BeautifulSoup, CSV, and the regular expression library. All you need to do to import those is just to run that code block there. And since we're working within a Jupyter Notebook, there's already an event loop that's running on the backend. So we cannot start a new event loop. For this reason, we need to run nest asyncio. What this module does is that it patches asyncio to allow a nested use of asyncio. So first we'll do a pip install. And we want to do a pip install of nest-asyncio. and then let's import nest_asyncio, and we'll call it Supply Function. So to do that, we'll say nest_asyncio.apply, and we'll run this. Next, let's write an asynchronous python function that scrapes all of the links from a given webpage's HTML content and saves them into a CSV file. This can be especially useful for gathering data for web analysis, SEO monitoring, or even just cataloging content. So we'll begin by defining our asynchronous function, which we will call scrape and save links. We'll use the async keyword to define an asynchronous function. So we'll say async def scrape_and_save_links(text):. Asynchronous functions are part of asynchronous programming and they allow us to handle long waiting operations like network requests more efficiently. They let other parts of your program run while waiting for these operations to complete. Inside the function, the first thing we'll do is parse the HTML content with BeautifulSoup. So we'll say soup equal to, call the BeautifulSoup constructor. We'll pass in our text object and we'll define the parser as HTML.parser. Now, we want to save these links to a file. To do this, we'll open a file in append mode. If the file doesn't exist, it will be created. If it does exist, we'll add to the end of it. We don't want to add any unintended new lines. So we'll set new line equal to a blank string, file equal to open, and then we will pass a string that reads CSV_file. The next parameter will be a string that reads A. And then lastly, our new line needs to be equal to an empty string. With the file opened, we create a CSV writer object. This object is responsible for converting our links into a format that's suitable for a CSV file, which by convention uses commas to separate items. So here we'll say, let me move this up a little. Okay, writer equal to CSV.writer. And we pass in our file and we just set our delimiter equal to a string that's got a comma in it. Next we'll loop through all the A tags in our Soup object. The final method looks for these tags and with the attrs parameter, we specify a regular expression to match the href attributes that start with HTTP. This way, we're only getting actual web links. So here we'll say for link in Soup.findall we'll pass in our A tag and then we'll say attrs equal to dictionary that contains href. And then we'll pass our regular expressions compile function. So re.compile, and we'll ask it to look for HTTP. The colon at the end of this. Each link we find is extracted using link.get('href'). Each link we find is extracted using link.get('href'). And we use our CSV writer to write this link into our file. So we'll say link is equal to link.get('href') and then writer.writerow and we'll pass in our link. And lastly, we need to close the file. Failing to close the file could lead to data not being written correctly or the file being left open unnecessarily, which is a resource link. So to close the file, we will write file.close, and then let's just run this whole thing. Okay, so here we have our function. Next we're going to define another piece of our web scraping toolkit, which will be the fetch function. This asynchronous functions responsible for making web request and then passing the content it retrieves to our scrape and save links function. Let's start by defining our function with the async keyword. This indicates that it's an asynchronous function and it allows us to use the await inside the function, which is essential for performing asynchronous IO operations. So we'll call the fetch function and we want to fetch our session and the URL. Now, we'll enter a try block. This is where we'll perform our web request. The reason we use a try block is because network operations are unpredictable. There might be connectivity issues, the server might not respond, or there could be a myriad of other issues that could arise. So we'll say try... Inside the try block, we use asynchronous context manager to send a get request to the URL we passed into the function. The session.get method is an asynchronous method, so we'll use the async with statement. We'll say async with session.get(url) as response. One thing I want to point out here is that the async with statement ensures that the session is properly closed after we're done with it even if an error does occur. Once we have the response from our get request, we want to retrieve the text of that page. We do this with await response.text. The await keyword is used to wait for the operation to complete without blocking the entire program. So we will say text is equal to await response.text. With the text of the response in hand, we now want to scrape the links from it. Here we use the asyncio create task to kick up our scrape and save links function. This function call creates a new task that runs concurrently with other tasks including the main program. So we'll say task is equal to asyncio.create_task, and then scrape and save links(text). We don't want to move on until we've actually scraped and saved the links, so we await the task to ensure it completes before proceeding. This is a key point, even though we're doing things concurrently, sometimes we need to wait for one task to finish before starting another. So we'll say await task here. And lastly, we have an except block. This is our safety net. If anything goes wrong with a network request or the scraping, instead of crashing our program, we catch the exception and print out the error message. This is just a general best practice for debugging and ensuring the robustness of your program. So we'll say except exception as e: print(str) passing e, and that's our fetch function. Oh! Okay. Looks like there's a syntax error. So let me take this async in and move it over. Move this line over this line. Tab it over, tab await over. Okay. Just clean up the indentations a bit. Okay. Yeah. So it was just a matter of cleaning up the indentations and that's our fetch function. It's designed to handle web request asynchronously, scrape the content for links and manage errors gracefully. With asyncio, this function will work efficiently as part of an asynchronous python application, fetching data and processing it without blocking other operations. Now we're ready to write a function that orchestrates the whole web scraping operation. We'll name this function scrape, and its job will be to manage multiple URLs and ensure that we fetch and process them concurrently. This is where the true power of asynchronous programming lies handling multiple IO bound tasks at once without waiting unnecessarily for each one to complete before starting the next. Here's how we do it. First we define our asynchronous function. So we say async def scrape, which will accept a list of URLs to process. Within this function, we'll initiate an empty list named tasks. This list will store the future tasks that we will create and then execute concurrently. Each task will be a web scraping operation for a single URL. So here we'll say tasks equal to, and we'll just put an empty list. Next we set up an asynchronous context manager using aiohttp client session. Aiohttp is an asynchronous HTTP client for Python, which allows us to make multiple HTTP requests concurrently by using async with, we ensure that the session is closed automatically once all operations within the block are completed. So we'll say async with aiohttp.ClientSession as session and colon. Now, we loop over each URL in the URL's list. For each URL we call the previously defined fetch function, which fetches the URL's content and processes it. We append the resulting task, a coroutine object to our tasks list. So we'll say for URL in URLs, tasks.append(fetch(session,url)). After we have iterated through all the URLs and created a task for each, we use asyncio.gather to run all of these tasks concurrently. Asyncio.gather takes a list of coroutines and schedules them to run concurrently by prefixing tasks with an asterisk. We're unpacking the list so that gather receives individual tasks as arguments. So we will say await asyncio.gather, and then we'll pass in an asterisk and then tasks. Now let's make sure our indentation is correct here. So this should actually be moved up one. Other than that, we need to also add a colon here. And then it looks pretty good, so I'll run it. Okay, no problems. One thing I want to point out here is that the await keyword is crucial. It means that the scrape function will wait until all the fetch tasks have been completed. Each fetch task involves sending a request to a URL, getting the response and then passing that response to scrape and save links, which saves the links into a CSV file. Once await asyncio.gather tasks completes, we know that all the URLs have been processed and the links have been saved. So here we are at the concluding portion of our web scraping session. Up to this point, we've built all of the individual components we need for our web scraping application. We have the scrape and save links function to extract links from the HTML content and save them to a CSV file. The fetch function to get the HTML content from our URL and the scrape function to manage all of our fetch calls concurrently. Now it's time to use them. Let's create a object called URLs and we're going to set it equal to a list of URLs. Let's make the first URL, https://analytics.usa.gov. And then our second URL will be python.org. So we'll say https://www.python.org. And lastly about we use LinkedIn, so https://www.linkedin.com. I'm going to do a forward slash just to make sure all of our ducks in a row here, clean up the formatting. And the next line is where we actually start our scraping operation. So we'll say asyncio.run(scrape), where URLs is equal to URLs. Those are the URLs we wrote into the list above. And run this. Okay, it looks like I missed the N here. It should be asyncio. Okay, run this. So it looks like it's found some objects that are not callable, but we should still have all of the links saved in the CSV file. So let's go ahead and go into our explorer here and look for a CSV file. Okay, so it looks like we have a problem with one of our functions. So what I'm going to do is I'm going to go back up and just check this syntax really quickly. And what I can see here is that this indentation needs to be moved out. And then also this A needs to be changed to a capital A. And then we will run this code block again, and okay, fix the problem. And now all of these links from these webpages have been scraped asynchronously. They will be saved in the CSV file. So we can look over here in the explorer section and you'll see we have a CSV file. And here is the CSV file with all of the links from each of the three pages that we referenced in our list.