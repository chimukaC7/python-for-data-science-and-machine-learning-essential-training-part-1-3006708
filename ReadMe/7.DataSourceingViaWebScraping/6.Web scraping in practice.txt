In the "Web scraping in practice" video, the key takeaways are:

Web Scraping Process: Demonstrates how to scrape a webpage using Python libraries like Beautiful Soup and urllib.
Extracting Data: Shows how to extract specific elements like links from a webpage and save the results to an external file.
Data Handling: Emphasizes the importance of data munging (cleaning and formatting) after scraping to handle stray characters and ensure data usability.

These steps will help you effectively scrape and manage web data for your projects.



Web scraping in practice
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Let me show you web scraping and action. In the following demonstration, I'm going to show you how to scrape a webpage and then save your results to an external file. Let's get started. So your Jupyter Notebook is coming with the standard libraries for beautiful soup already loaded in it, and you've also got URL Lib, and regular expression library preloaded. So all you need to do is run this and what we're going to do is we're going to scrape a page from analytics.usa.gov. So to start, we're going to create an R variable, and we're going to say R is equal to urllib.request.urlopen, and then we're going to pass in a string that is the URL we want to be having data read from. So that's going to be https://analytics.usa.gov. And then we're going to call the read method off of this whole thing. And next, we're going to create a soup variable, and we're going to set it equal to the beautiful soup constructor. And then we want to pass, our first perimeter is going to be our R variable. And then our second parameter will be the HTML parser to tell Beautiful Soup to use the HTML parser to read the data. And we'll call the type function and pass in our soup object just to double check the type of our soup object, which of course, should be a beautiful soup. So I'll run this and yeah, there we have it. It's a beautiful soup object. So just remember here that you can use any web link you want to basically scrap data from any webpage on the internet. Now, I'm going to show you how to script a webpage and save your results. First, let's start by printing out our soup object. So we're going to call the print function, pass in soup.prettify. And then let's just take a look at the first 100 characters. I'll run this and okay, so we're seeing the very tip top of the webpage that's located at the analytics.us.gov url. Next, what we should do is just use the find all function to find all of the A tags and then retrieve from within them the A tag values from within those. To do that, we're going to create a loop. So it'll be a for loop and for each link in soup.find all, we're looking for the A tags. And then we want to print, oh, looks like I forgot a colon here at the end of this. Okay, so then for each of those, we want to print the link and then we need to get the A. So that's going to be link.get. And within this get method, we're going to pass a perimeter that reads href and run this. Okay, cool. So now we have a list of links. And what this has done is it's actually gone through the analytics.usa.gov webpage, and it's looped through all of the text on that page and basically printed out only the web links. But if you wanted to see what that entire body of text actually looks like, then you can use the get text method. So let's just try that out really quickly. We're going to say print, and we'll pass in our soup object. And off of that, we will call the get Text method and run this. Okay, so there is a lot of content. So what I'm going to do is just click this option as view it as a scrollable element. And here is the body of text that's sitting on that webpage. We have scraped it directly from the webpage in real-time. The benefit of using this for loop that we created here is it basically went through all of the text and got us exactly what we needed, which was the links instead of us having to kind of pick and choose through the body of text. And in this case, I'm not even seeing the hyperlinks. So if you just want to scrape links from a webpage, then you might as well use this method up here. Let's go ahead and just prettify this, so we can kind of take a look at it more easily. To do that, we'll call the print function, we'll pass in our soup object, soup.prettify and then let's just look at the first 1,000 characters. So we'll, 0:1000 and run this. Okay, so here, we're seeing the links printed out, and it's a lot more manageable to read, and it's a lot easier to read than the output from above. Now, what I want to do is create a for loop to pass through our soup object and find all of the A tags that have an attribute of href. So we'll say for link in soup.find all. And the first parameter will be a string that reads A with attribute equal to href. So we're going to say attris equal to, and then we're going to create a dictionary, and we're going to pass a string that reads href. And then for all of these tags that are returned, we want the loop to match against them irregular expression that reads HTTPS and print out only those. So to make that happen, we're going to call the compile function from the regular expression library, so that's re.compile. And within that, we're going to pass a string that reads HTTP. And then for any results that match this expression, we just print them out. So we're going to call the print function and pass in our link. All right, and I'm going to check the syntax here 'cause it looks like something is off. Yeah, I am missing the closing parentheses here to close out the tuple. Okay, so I'm going to run this, and that's a long list of tags with links within them. Let's just take a look at what the data type is here. So to do that, we'll say, type and pass in our link object. So as you can see, this is actually a tag object, and what we basically have is we have all of our A tags that have an attribute of href and also have an HTTP batch within them. It isn't useful for you to have this result stuck within a Jupyter Notebook though, so you'll want to know how to actually save this as an external file. To do that, we're going to create a new text file called parsed data. And so, we're going to create a file variable, and say, file is equal to, and then we'll call the open function. We'll pass in a string that reads parsed_data.text, and the second parameter will be a string that reads W. What that's doing is it's telling Python that we want to write into that text file, so W stands for write. And then what we want to do is for each of the links that was just printed out, we want to print those now into the parsed data text file. So now, what we actually need to do is that we can just go ahead and copy this code from above of our for loop. We'll copy this and then we'll just reuse it for efficiency's sake. I'm going to paste it here, and it's just performing the same operation now as it did before. But instead of printing out here into Jupyter, it's going to go ahead, and it's going to generate a soup link, and that's going to be a string. So instead of printing the link, it's going to create a new variable called soup link. And that link is going to be equal to a string. And this string is going to be derived from each link within the soup object. So what I'm going to do here is that I'm going to move this down, and I'm going to say soup_link is equal to str and then I'm going to pass in a link there. And then for each soup link, we want to print that out. So we need to update this print function, so that it's printing soup_link. And then we're going to write into the file. So we're going to say file.write, and we're going to pass in our soup link, soup_link. And what this loop is going to do is that it's going to pass it the entire beautiful soup object, and it's going to find all of the links and print them out until it finds no more links, and then it's going to flush the file and close the file. So to make that happen, we say file.flush, file.flush, and file.close. And let me just check really quick for any issues with the syntax. Okay, we'll run this and great. Okay, so we have a list of tags with the links inside of them. So essentially, this is what should be written into our text file, correct? So let's check that. I could show you the shortcut of where to find the file, but I also want to show you how to use the present working directory command to show you where to go to retrieve that text file. So you need to pull up your present working directory, and if you just call the command, %pwd and run this, it's going to tell you exactly where you can go to find the text file that was just printed. So it's in our folder that we're actually working within in code spaces. The extension is printed out here for us. And then of course, the shortcut is you could just go up to this explorer, and since we're working in the notebooks folder, you can actually just find the file written here. So let's look at that and then you can see, okay, here are our links. It looks like they're not quite as nicely formatted, but it's the same information that has been printed out in the Jupyter Notebook, except for now it's a text file, which can, in times, be more convenient. The only thing I would mention here is that you can still see a bunch of stray tags. And a lot of times when you're doing web scraping, no matter how much data formatting you do, there's always these stray characters. Basically, a lot of times, there are data processing requirements after you scrape the data. So expect to spend some time data munging after you do web scraping. But if you ever find yourself again in a position where you can't get data from a website because it's been placed on different pages or in weird formatting, remember how to use beautiful soup to scrape the data for you. And in the next lecture, you're actually going to learn how to build a whole web scraping application that will print out links for you in such a way that they're perfectly formatted. And so, stay tuned because that's next.